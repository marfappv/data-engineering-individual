{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing NFT Price Class Prediction System with SageMaker Built-In Algorithm\n",
    "_**Making Price Class Suggestion for NFT Using Factorization Machines**_\n",
    "\n",
    "--- \n",
    "\n",
    "*This work is based on content from [Implementing Recommender System notebook](https://github.com/aws-samples/sagemaker-ml-workflow-with-apache-airflow/blob/master/notebooks/amazon-video-recommender_using_fm_algo.ipynb)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Explore](#Explore)\n",
    "  1. [Clean](#Clean)\n",
    "  1. [Prepare](#Prepare)\n",
    "1. [Model Training](#Model-Training)\n",
    "1. [Model Inference](#Model-Inference)\n",
    "  1. [Real-Time Inference](#Real-Time-Inference)\n",
    "  1. [Batch Inference](#Batch-Inference)\n",
    "1. [Evaluate Model Performance](#Evaluate-Model-Performance)\n",
    "1. [Model Tuning](#Model-Tuning)\n",
    "1. [Wrap-up](#Wrap-up)\n",
    "  1. [Clean-Up](#Clean-up-(optional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.  The minimal required dataset for this is a history of user item ratings.  In our case, we'll use 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos. More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "Matrix factorization has been the cornerstone of most user-item prediction models.  This method starts with the large, sparse, user-item ratings in a single matrix, where users index the rows, and items index the columns.  It then seeks to find two lower-dimensional, dense matrices which, when multiplied together, preserve the information and relationships in the larger matrix.\n",
    "\n",
    "![image](./factorization.png)\n",
    "\n",
    "Matrix factorization has been extended and generalized with deep learning and embeddings.  These techniques allows us to introduce non-linearities for enhanced performance and flexibility.  This notebook will fit a neural network-based model to generate recommendations for the Amazon video dataset.  It will start by exploring our data in the notebook, training a model on the data and fit our model using a SageMaker managed training cluster.  We'll then deploy to an endpoint and check our method.\n",
    "\n",
    "We will also see how the tasks in the machine learning pipeline can be orchestrated and automated through Apache Airflow integration with Sagemaker.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.t2.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must setup local AWS configuration with a region supported by SageMaker.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000004?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtuner\u001b[39;00m \u001b[39mimport\u001b[39;00m HyperparameterTuner, ContinuousParameter\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000004?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39manalytics\u001b[39;00m \u001b[39mimport\u001b[39;00m HyperparameterTuningJobAnalytics, TrainingJobAnalytics\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000004?line=8'>9</a>\u001b[0m role \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39;49mget_execution_role()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000004?line=9'>10</a>\u001b[0m sess \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39mSession()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000004?line=10'>11</a>\u001b[0m smclient \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mSession()\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39msagemaker\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sagemaker/session.py:4470\u001b[0m, in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4458'>4459</a>\u001b[0m \u001b[39m\"\"\"Return the role ARN whose credentials are used to call the API.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4459'>4460</a>\u001b[0m \n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4460'>4461</a>\u001b[0m \u001b[39mThrows an exception if role doesn't exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4466'>4467</a>\u001b[0m \u001b[39m    (str): The role ARN\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4467'>4468</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4468'>4469</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sagemaker_session:\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4469'>4470</a>\u001b[0m     sagemaker_session \u001b[39m=\u001b[39m Session()\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4470'>4471</a>\u001b[0m arn \u001b[39m=\u001b[39m sagemaker_session\u001b[39m.\u001b[39mget_caller_identity_arn()\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=4472'>4473</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m:role/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m arn:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sagemaker/session.py:126\u001b[0m, in \u001b[0;36mSession.__init__\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket, settings)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=122'>123</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_client \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=123'>124</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings \u001b[39m=\u001b[39m settings\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=125'>126</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=126'>127</a>\u001b[0m     boto_session\u001b[39m=\u001b[39;49mboto_session,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=127'>128</a>\u001b[0m     sagemaker_client\u001b[39m=\u001b[39;49msagemaker_client,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=128'>129</a>\u001b[0m     sagemaker_runtime_client\u001b[39m=\u001b[39;49msagemaker_runtime_client,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=129'>130</a>\u001b[0m     sagemaker_featurestore_runtime_client\u001b[39m=\u001b[39;49msagemaker_featurestore_runtime_client,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=130'>131</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sagemaker/session.py:149\u001b[0m, in \u001b[0;36mSession._initialize\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=146'>147</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_region_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=147'>148</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_region_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=148'>149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=149'>150</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMust setup local AWS configuration with a region supported by SageMaker.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=150'>151</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=152'>153</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client \u001b[39m=\u001b[39m sagemaker_client \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mclient(\u001b[39m\"\u001b[39m\u001b[39msagemaker\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/sagemaker/session.py?line=153'>154</a>\u001b[0m prepend_user_agent(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client)\n",
      "\u001b[0;31mValueError\u001b[0m: Must setup local AWS configuration with a region supported by SageMaker."
     ]
    }
   ],
   "source": [
    "bucket = 's3a://airflow-sagemaker-cd6fd720'\n",
    "#prefix = 'sagemaker/fm-recsys'\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics, TrainingJobAnalytics\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/nbformat/current.py:15: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  warnings.warn(\"\"\"nbformat.current is deprecated.\n"
     ]
    }
   ],
   "source": [
    "#Essentials\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from pandas.api.types import CategoricalDtype\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np; np.random.seed(2022)\n",
    "import random\n",
    "\n",
    "#Image creation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import pyplot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#Image display\n",
    "from IPython.display import Image as image\n",
    "from IPython.display import display\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Metrics of accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "#Other\n",
    "from nbformat import current\n",
    "import itertools as it\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from scipy.sparse import lil_matrix\n",
    "import boto3\n",
    "import json\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot aesthetics\n",
    "sns.set(color_codes=True)\n",
    "sns.set_context('paper')\n",
    "five_thirty_eight = [\"#30a2da\", \"#fc4f30\", \"#e5ae38\", \"#6d904f\", \"#8b8b8b\",]\n",
    "sns.set_palette(five_thirty_eight)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "### Explore\n",
    "\n",
    "Let's start by bringing in our dataset from an S3 public bucket.  As mentioned above, this contains 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "_Note, because this dataset is over a half gigabyte, the load from S3 may take ~10 minutes.  Also, since Amazon SageMaker Notebooks start with a 5GB persistent volume by default, and we don't need to keep this data on our instance for long, we'll bring it to the temporary volume (which has up to 20GB of storage)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir /tmp/recsys/\n",
    "#!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a Pandas DataFrame so that we can begin to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import fastparquet as fp\n",
    "s3 = s3fs.S3FileSystem()\n",
    "fs = s3fs.core.S3FileSystem()\n",
    "\n",
    "s3_path = \"s3a://data-eng-ind/parquet-files/all_collections_API.parquet\"\n",
    "all_paths_from_s3 = fs.glob(path=s3_path)\n",
    "\n",
    "myopen = s3.open\n",
    "#use s3fs as the filesystem\n",
    "fp_obj = fp.ParquetFile(all_paths_from_s3,open_with=myopen)\n",
    "#convert to pandas dataframe\n",
    "df = fp_obj.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_status</th>\n",
       "      <th>only_proxied_transfers</th>\n",
       "      <th>is_subject_to_whitelist</th>\n",
       "      <th>opensea_buyer_fee_basis_points</th>\n",
       "      <th>opensea_seller_fee_basis_points</th>\n",
       "      <th>featured</th>\n",
       "      <th>hidden</th>\n",
       "      <th>require_email</th>\n",
       "      <th>image_url</th>\n",
       "      <th>large_image_url</th>\n",
       "      <th>...</th>\n",
       "      <th>owner_number</th>\n",
       "      <th>day_avg_price</th>\n",
       "      <th>month_avg_price</th>\n",
       "      <th>week_avg_price</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>total_supply</th>\n",
       "      <th>average_price</th>\n",
       "      <th>max_price</th>\n",
       "      <th>min_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_requested</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>7356207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not_requested</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3050053.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not_requested</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>7471608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_requested</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>5909669.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_requested</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>7396100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  collection_status  only_proxied_transfers  is_subject_to_whitelist  \\\n",
       "0     not_requested                   False                    False   \n",
       "1     not_requested                   False                    False   \n",
       "2     not_requested                   False                    False   \n",
       "3     not_requested                   False                    False   \n",
       "4     not_requested                   False                    False   \n",
       "\n",
       "  opensea_buyer_fee_basis_points opensea_seller_fee_basis_points  featured  \\\n",
       "0                              0                             250     False   \n",
       "1                              0                             250     False   \n",
       "2                              0                             250     False   \n",
       "3                              0                             250     False   \n",
       "4                              0                             250     False   \n",
       "\n",
       "   hidden  require_email image_url large_image_url  ... owner_number  \\\n",
       "0   False          False      None            None  ...    7356207.0   \n",
       "1   False          False      None            None  ...    3050053.0   \n",
       "2   False          False      None            None  ...    7471608.0   \n",
       "3   False          False      None            None  ...    5909669.0   \n",
       "4   False          False      None            None  ...    7396100.0   \n",
       "\n",
       "  day_avg_price month_avg_price  week_avg_price total_volume total_sales  \\\n",
       "0           0.0             0.0             0.0          0.0         0.0   \n",
       "1           0.0             0.0             0.0          0.0         0.0   \n",
       "2           0.0             0.0             0.0          0.0         0.0   \n",
       "3           0.0             0.0             0.0          0.0         0.0   \n",
       "4           0.0             0.0             0.0          0.0         0.0   \n",
       "\n",
       "  total_supply average_price max_price  min_price  \n",
       "0          1.0           0.0       0.0          0  \n",
       "1          1.0           0.0       0.0          0  \n",
       "2          2.0           0.0       0.0          0  \n",
       "3         50.0           0.0       0.0          0  \n",
       "4          1.0           0.0       0.0          0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(how='all', axis=1)\n",
    "df = df.dropna(how='all')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>owner_number</th>\n",
       "      <td>4872.0</td>\n",
       "      <td>6.077133e+06</td>\n",
       "      <td>1.773308e+06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5324539.0</td>\n",
       "      <td>6939835.0</td>\n",
       "      <td>7422735.25</td>\n",
       "      <td>7492468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_avg_price</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>4.297307e-03</td>\n",
       "      <td>1.723171e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_avg_price</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7.524136e-03</td>\n",
       "      <td>2.387651e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week_avg_price</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7.524136e-03</td>\n",
       "      <td>2.387651e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_volume</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>1.112951e-02</td>\n",
       "      <td>3.685486e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_sales</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>5.306667e-02</td>\n",
       "      <td>2.373591e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_supply</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>3.439133e+01</td>\n",
       "      <td>3.532359e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_price</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7.524136e-03</td>\n",
       "      <td>2.387651e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_price</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>6.522798e-02</td>\n",
       "      <td>3.397752e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_price</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count          mean           std   min        25%  \\\n",
       "owner_number     4872.0  6.077133e+06  1.773308e+06  25.0  5324539.0   \n",
       "day_avg_price    7500.0  4.297307e-03  1.723171e-01   0.0        0.0   \n",
       "month_avg_price  7500.0  7.524136e-03  2.387651e-01   0.0        0.0   \n",
       "week_avg_price   7500.0  7.524136e-03  2.387651e-01   0.0        0.0   \n",
       "total_volume     7500.0  1.112951e-02  3.685486e-01   0.0        0.0   \n",
       "total_sales      7500.0  5.306667e-02  2.373591e+00   0.0        0.0   \n",
       "total_supply     7500.0  3.439133e+01  3.532359e+02   0.0        1.0   \n",
       "average_price    7500.0  7.524136e-03  2.387651e-01   0.0        0.0   \n",
       "max_price        7500.0  6.522798e-02  3.397752e+00   0.0        0.0   \n",
       "min_price        7500.0  0.000000e+00  0.000000e+00   0.0        0.0   \n",
       "\n",
       "                       50%         75%        max  \n",
       "owner_number     6939835.0  7422735.25  7492468.0  \n",
       "day_avg_price          0.0        0.00       12.5  \n",
       "month_avg_price        0.0        0.00       12.5  \n",
       "week_avg_price         0.0        0.00       12.5  \n",
       "total_volume           0.0        0.00       25.0  \n",
       "total_sales            0.0        0.00      204.0  \n",
       "total_supply           2.0        5.00    10169.0  \n",
       "average_price          0.0        0.00       12.5  \n",
       "max_price              0.0        0.00      275.0  \n",
       "min_price              0.0        0.00        0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive statistics of the transposed table\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     15\n",
      "float64     9\n",
      "bool        6\n",
      "int64       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show number of all existing data types in the dataframe\n",
    "print(df.dtypes.astype(str).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a PostgreSQL table and load into a pandas DataFrame\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine('postgresql+psycopg2://test:@127.0.0.1', pool_recycle=3600)\n",
    "\n",
    "# Connect to PostgreSQL server\n",
    "dbConnection = alchemyEngine.connect()\n",
    "\n",
    "# Read data from PostgreSQL database table and load into a DataFrame instance\n",
    "df1 = pd.read_sql(\"select * from nfts.assets\", dbConnection)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Close the database connection\n",
    "dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"opensea.ckmusmy93z05.eu-west-2.rds.amazonaws.com\" (35.176.71.104), port 5432 failed: Operation timed out\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000146?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpsycopg2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpg\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000146?line=2'>3</a>\u001b[0m engine \u001b[39m=\u001b[39m pg\u001b[39m.\u001b[39;49mconnect(\u001b[39m\"\u001b[39;49m\u001b[39mdbname=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mopensea\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m user=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmarfapopova21\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m host=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mopensea.ckmusmy93z05.eu-west-2.rds.amazonaws.com\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m port=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m5432\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m password=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mqwerty123\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Marfa-Popova/data_eng_ind/ML_training/ML_training.ipynb#ch0000146?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_sql(\u001b[39m'\u001b[39m\u001b[39mselect * from nfts.assets\u001b[39m\u001b[39m'\u001b[39m, con\u001b[39m=\u001b[39mengine)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/psycopg2/__init__.py?line=118'>119</a>\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/psycopg2/__init__.py?line=120'>121</a>\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/psycopg2/__init__.py?line=121'>122</a>\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/psycopg2/__init__.py?line=122'>123</a>\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/psycopg2/__init__.py?line=123'>124</a>\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server at \"opensea.ckmusmy93z05.eu-west-2.rds.amazonaws.com\" (35.176.71.104), port 5432 failed: Operation timed out\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "engine = pg.connect(\"dbname='opensea' user='marfapopova21' host='opensea.ckmusmy93z05.eu-west-2.rds.amazonaws.com' port='5432' password='qwerty123'\")\n",
    "df1 = pd.read_sql('select * from nfts.assets', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset includes information like:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "\n",
    "For this example, let's limit ourselves to `customer_id`, `product_id`, and `star_rating`.  Including additional features in our recommendation system could be beneficial, but would require substantial processing (particularly the text data) which would take us beyond the scope of this notebook.\n",
    "\n",
    "*Note: we'll keep `product_title` on the dataset to help verify our recommendations later in the notebook, but it will not be used in algorithm training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'star_rating', 'product_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most people haven't seen most videos, and people rate fewer videos than we actually watch, we'd expect our data to be sparse.  Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  Let's look at some basic percentiles to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reviews['customer_id'].value_counts()\n",
    "products = reviews['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "product_q = pd.DataFrame(zip(quantiles, products.quantile(quantiles)), columns=[\"quantile\", \"products\"])\n",
    "customer_q = pd.DataFrame(zip(quantiles, customers.quantile(quantiles)), columns=[\"quantile\", \"customers\"])\n",
    "# product_q.tail(10)\n",
    "# customer_q.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axp = sns.barplot(x=\"quantile\", y=\"products\", data=product_q, palette=five_thirty_eight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axc = sns.barplot(x=\"quantile\", y=\"customers\", data=customer_q, palette=five_thirty_eight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only about 5% of customers have rated 5 or more videos, and only 25% of videos have been rated by 9+ customers.\n",
    "\n",
    "### Clean\n",
    "\n",
    "Let's filter out this long tail and remove any duplicate reviews (same product and customer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "print(\"# of records before removing the long tail = {:10d}\".format(reviews.shape[0]))\n",
    "reduced_df = reviews.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))\n",
    "print(\"# of records after  removing the long tail = {:10d}\".format(reduced_df.shape[0]))\n",
    "reduced_df = reduced_df.drop_duplicates(['customer_id', 'product_id'])\n",
    "print(\"# of records after  removing duplicates    = {:10d}\".format(reduced_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'customer': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'product': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the feature dimension size whch will required for preparing the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_customer = reduced_df['customer'].max() + 1\n",
    "nb_products = reduced_df['product'].max() + 1\n",
    "feature_dim = nb_customer + nb_products\n",
    "print(nb_customer, nb_products, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim down the data set to include only customer, product, star_rating which is all we need for the training algorithm to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = reduced_df[['customer', 'product', 'star_rating']]\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "We will be using SageMaker's implementation of Factorization Machines (FM) for building a recommender system. The algorithm expects float32 tensors in protobuf whereas the data sets are pandas dataframe on disk. Most of the conversion effort is handled by the Amazon SageMaker Python SDK.\n",
    "\n",
    "The FM algorithm will utilize sparse input and since the data sets are dense matrix, it has to be converted a sparse matrix with one-hot encoded feature vectors with customers and products. Thus, each sample in the data set will be a wide boolean vector with 178729 feature space (140344 customer + 38385 products) with only two values set to 1 with respect to the customer and product.\n",
    "\n",
    "Following are the next steps\n",
    "\n",
    "1. Split the cleaned data set into train and test data sets.\n",
    "2. For each set, build a sparse matrix with one-hot encoded feature vectors (customer + products) and a label vector with star ratings.\n",
    "3. Convert both the sets to protobuf encoded files.\n",
    "4. Copy these files to an Amazon S3 bucket.\n",
    "5. Configure and run a Factorization Machines training job on Amazon SageMaker.\n",
    "6. Deploy the corresponding model to an endpoint.\n",
    "7. Run predictions on test data set and validate\n",
    "\n",
    "#### Split into Training and Test Data Sets\n",
    "\n",
    "Let's start by [splitting](https://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html) in training, validation and test sets.  This will allow us to estimate the model's accuracy on videos our customers rated, but wasn't included in our training. We will use validation data set specifically for tuning model hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validate_df, test_df = np.split(\n",
    "    product_df.sample(frac=1), \n",
    "    [int(.6*len(product_df)), int(.8*len(product_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of rows in the training data set   = {:10d}\".format(train_df.shape[0]))\n",
    "print(\"# of rows in the validation data set = {:10d}\".format(validate_df.shape[0]))\n",
    "print(\"# of rows in the test data set       = {:10d}\".format(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the feature dimensions by adding total number of (unique) customers and products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature dimension\n",
    "all_df = pd.concat([train_df, validate_df, test_df])\n",
    "nb_customer = np.unique(all_df['customer'].values).shape[0]\n",
    "nb_products = np.unique(all_df['product'].values).shape[0]\n",
    "feature_dim = nb_customer + nb_products\n",
    "print(\"# of customers = {:10d}\".format(nb_customer))\n",
    "print(\"# of products  = {:10d}\".format(nb_products))\n",
    "print(\"# of features  = {:10d}\".format(feature_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Sparse One-Hot Encoded Matrix\n",
    "\n",
    "Our training matrix is now even sparser: Of all 183,833,321,511 values (1028559 rows * 178729 columns), only 2,057,118 are non-zero (1,028,559*2). In other words, the matrix is 99.99% sparse. Storing this as a dense matrix would be a massive waste of both storage and computing power. To avoid this, use a scipy.lil_matrix sparse matrix for features and a numpy array for ratings.\n",
    "\n",
    "Let's define a function that takes the data set and returns a sparse feature matrix and numpy array with ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix(df, nb_rows, nb_customer, nb_products):\n",
    "    # dataframe to array\n",
    "    df_val = df.values\n",
    "\n",
    "    # determine feature size\n",
    "    nb_cols = nb_customer + nb_products\n",
    "    print(\"# of rows = {}\".format(str(nb_rows)))\n",
    "    print(\"# of cols = {}\".format(str(nb_cols)))\n",
    "\n",
    "    # extract customers and ratings\n",
    "    df_X = df_val[:, 0:2]\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((nb_rows, nb_cols)).astype('float32')\n",
    "    df_X[:, 1] = nb_customer + df_X[:, 1]\n",
    "    coords = df_X[:, 0:2]\n",
    "    X[np.arange(nb_rows), coords[:, 0]] = 1\n",
    "    X[np.arange(nb_rows), coords[:, 1]] = 1\n",
    "\n",
    "    # create label with ratings\n",
    "    Y = df_val[:, 2].astype('float32')\n",
    "\n",
    "    # validate size and shape\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    assert X.shape == (nb_rows, nb_cols)\n",
    "    assert Y.shape == (nb_rows, )\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert training data set to one-hot encoded sparse matrix\")\n",
    "train_X, train_Y = convert_sparse_matrix(train_df, train_df.shape[0], nb_customer, nb_products)\n",
    "print(\"Convert validation data set to one-hot encoded sparse matrix\")\n",
    "validate_X, validate_Y = convert_sparse_matrix(validate_df, validate_df.shape[0], nb_customer, nb_products)\n",
    "print(\"Convert test data set to one-hot encoded sparse matrix\")\n",
    "test_X, test_Y = convert_sparse_matrix(test_df, test_df.shape[0], nb_customer, nb_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Protobuf format and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Sagemaker's utility function [`write_spmatrix_to_sparse_tensor`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/amazon/common.py) to convert scipy sparse matrix to protobuf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_protobuf(X, Y, bucket, key):\n",
    "    \"\"\"Converts features and predictions matrices to recordio protobuf and\n",
    "       writes to S3\n",
    "\n",
    "    Args:\n",
    "        X:\n",
    "          2D numpy matrix with features\n",
    "        Y:\n",
    "          1D numpy matrix with predictions\n",
    "        bucket:\n",
    "          s3 bucket where recordio protobuf file will be staged\n",
    "        prefix:\n",
    "          s3 url prefix to stage prepared data to use for training the model\n",
    "        key:\n",
    "          protobuf file name to be staged\n",
    "\n",
    "    Returns:\n",
    "        s3 url with key to the protobuf data\n",
    "    \"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}'.format(key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_path = save_as_protobuf(train_X, train_Y, bucket, 'prepare/train/train.protobuf')\n",
    "print(\"Training data set in protobuf format uploaded at {}\".format(s3_train_path))\n",
    "s3_val_path = save_as_protobuf(validate_X, validate_Y, bucket, 'prepare/validate/validate.protobuf')\n",
    "print(\"Validation data set in protobuf format uploaded at {}\".format(s3_val_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will chunk the test data to avoid the payload size issues when performing batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(x, batch_size):\n",
    "    \"\"\"split array into chunks of batch_size\n",
    "    \"\"\"\n",
    "    chunk_range = range(0, x.shape[0], batch_size)\n",
    "    chunks = [x[p: p + batch_size] for p in chunk_range]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_chunks = chunk(test_X, 10000)\n",
    "test_y_chunks = chunk(test_Y, 10000)\n",
    "N = len(test_x_chunks)\n",
    "for i in range(N):\n",
    "    test_data = save_as_protobuf(\n",
    "        test_x_chunks[i],\n",
    "        test_y_chunks[i],\n",
    "        bucket,\n",
    "        \"prepare/test/test_\" + str(i) + \".protobuf\")\n",
    "    print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. We'll use the Amazon SageMaker Python SDK to kick off training and monitor status until it is completed. In this example that takes between 4-7 minutes for 3-10 epochs. \n",
    "\n",
    "First, let's get the Sagemaker Factorization Machine container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'factorization-machines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next kick off the base estimator, making sure to pass in the necessary hyperparameters. Notice:\n",
    "\n",
    "- `feature_dim` is set to 178729, which is the number of customers + products in the training data set.\n",
    "- `predictor_type` is set to 'regressor' since we are trying to predict the rating\n",
    "- `mini_batch_size` is set to 200. This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases.\n",
    "- `num_factors` is set to 64. Factorization machines find a lower dimensional representation of the interactions for all features. Making this value smaller provides a more parsimonious model, closer to a linear model, but may sacrifice information about interactions. Making it larger provides a higher-dimensional representation of feature interactions, but adds computational complexity and can lead to overfitting. In a practical application, time should be invested to tune this parameter to the appropriate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "output_location = 's3://{}/train/'.format(bucket)\n",
    "s3_train_path = 's3://{}/prepare/train/train.protobuf'.format(bucket)\n",
    "s3_val_path = 's3://{}/prepare/validate/validate.protobuf'.format(bucket)\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   role, \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c5.4xlarge',\n",
    "                                   output_path=output_location,\n",
    "                                   sagemaker_session=sess)\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=feature_dim,\n",
    "                      predictor_type='regressor',\n",
    "                      mini_batch_size=200,\n",
    "                      num_factors=512,\n",
    "                      bias_lr=0.02,\n",
    "                      epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.fit({'train': s3_train_path,'test': s3_val_path}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker built-in algorithms automatically compute and emit a variety of model training, evaluation, and validation metrics that can be captured from Cloudwatch using Sagemaker SDK. Since we are using FM built-in algorithm with predictor type as `regressor`, we can capture RMSE (root-mean-square error) of the model that measures the differences between the predicted values and the actual values.\n",
    "\n",
    "Let's capture the RMSE of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = fm._current_job_name\n",
    "metric_name = 'train:rmse:epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check current status of training job\n",
    "fm_training_job_result = smclient.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "status = fm_training_job_result['TrainingJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the training job has not been completed.')\n",
    "else:\n",
    "    print('The training job is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug-in the training job name and metrics to be captured\n",
    "metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name,metric_names=[metric_name]).dataframe()\n",
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = metrics_dataframe.plot(kind='line', figsize=(12,5), x='timestamp', y='value', style='b.', legend=False)\n",
    "plt.set_ylabel(metric_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of epochs increased, RMSE goes down which is a good sign that the predicted values are getting closer to the actual values from the test set. We can increase number of epochs or change the hyperparameters and try to tweak the model. Let's try to deploy this model and make predictions to see how close the predictions are. Then we can run a hyper-parameter tuning job to determine the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define some common utility functions here that will be used during inference and evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_protobuf(X, Y=None):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_X(df, nb_rows, nb_customer, nb_products):\n",
    "    # dataframe to array\n",
    "    df_val = df.values\n",
    "\n",
    "    # determine feature size\n",
    "    nb_cols = nb_customer + nb_products\n",
    "    \n",
    "    # extract customers and ratings\n",
    "    df_X = df_val[:,0:2]\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((nb_rows, nb_cols)).astype('float32')\n",
    "    df_X[:,1] = nb_customer + df_X[:,1]\n",
    "    coords = df_X[:,0:2]\n",
    "    X[np.arange(nb_rows), coords[:, 0]] = 1\n",
    "    X[np.arange(nb_rows), coords[:, 1]] = 1\n",
    "\n",
    "    # validate size and shape\n",
    "    assert X.shape == (nb_rows, nb_cols)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is trained, all it takes to deploy the model is a Sagemaker API call `deploy()` that creates the model package, sets up endpoint configuration and finally creates the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions could be done by sending HTTP POST requests from a separate web service, but to keep things easy, we'll just use the `.predict()` method from the SageMaker Python SDK. The API expects JSON or RecordIO format for  request and JSON for response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.content_type = 'application/x-recordio-protobuf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model with sample ratings from test data set using `predict()` API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pb = convert_to_protobuf(test_X[1000:1010]).getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = fm_predictor.predict(test_pb)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [round(r['score'], 2) for r in json.loads(response)['predictions']]\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(zip(test_Y[1000:1010], predicted), columns = [\"actual_rating\", \"predicted_rating\"])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will perform batch inference on the test data set prepared earlier (chunking into multiple protobuf files). To run batch transform, create a model package for the transform endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the model from the training estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model = fm.create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform batch inference on the test data set and save results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_transformer = fm_model.transformer(\n",
    "    instance_type='ml.c4.xlarge', \n",
    "    instance_count=1, \n",
    "    strategy=\"MultiRecord\", \n",
    "    output_path=\"s3://{}/transform/\".format(bucket)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_transformer.transform(\n",
    "    data=\"s3://{}/prepare/test/\".format(bucket), \n",
    "    data_type='S3Prefix', \n",
    "    content_type=\"application/x-recordio-protobuf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for transform job: ' + fm_transformer.latest_transform_job.job_name)\n",
    "fm_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inference results will be stored in a separate file for each test file chunk. Let's download the results from S3 and merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_s3(bucket, key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object( bucket, key)\n",
    "    content = obj.get()['Body'].read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for i in range(N):\n",
    "    key = 'transform/test_' + str(i) + '.protobuf.out'\n",
    "    response = download_from_s3(bucket, key)\n",
    "    result = [json.loads(row)[\"score\"] for row in response.split(\"\\n\") if len(row) > 0]\n",
    "    test_preds.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.array(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "Let's start by calculating a naive baseline to approximate how well our model is doing.  The simplest estimate would be to assume every user item rating is just the average rating over all ratings.\n",
    "\n",
    "*Note, we could do better by using each individual video's average, however, in this case it doesn't really matter as the same conclusions would hold.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive MSE:', np.mean((test_df['star_rating'] - np.mean(train_df['star_rating'])) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll calculate predictions for our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', np.mean((test_Y - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our neural network and embedding model produces substantially better results (~1.44 vs 1.13 on the mean square error).\n",
    "\n",
    "For recommender systems, subjective accuracy also matters.  Let's get some recommendations for a random user to see if they make intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_6 = reduced_df[reduced_df['customer'] == 6].sort_values(['star_rating', 'product'], ascending=[False, True])\n",
    "pd.concat((df_customer_6.head(10), df_customer_6.tail(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, user #6 seems to like sprawling dramamtic television series and sci-fi, but they dislike silly comedies.\n",
    "\n",
    "Now we'll loop through and predict user #6's ratings for every common video in the catalog, to see which ones we'd recommend and which ones we wouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_payload(cust_id, nb_customer, nb_products, product_index):\n",
    "    # prepare payload for user #6\n",
    "    c = [cust_id] * nb_products\n",
    "    p = product_index['product'].values\n",
    "    x = pd.DataFrame(zip(c,p))\n",
    "    p_x = convert_sparse_matrix_X(x, x.shape[0], nb_customer, nb_products)\n",
    "    x_pb = convert_to_protobuf(p_x)\n",
    "    return x_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pb = create_payload(6, nb_customer, nb_products, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using end-point created in Real-Time Inference\n",
    "response = fm_predictor.predict(x_pb)\n",
    "predictions = [round(r['score'], 2) for r in json.loads(response)['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({'product': product_index['product'],\n",
    "                            'prediction': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_cust_6 = df_customer_6.merge(predictions_df, on=['product'])[['customer', 'customer_id', 'product', 'product_id', 'product_title', 'star_rating', 'prediction']]\n",
    "df_results_cust_6.sort_values(['prediction', 'product'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our predicted highly rated shows have some well-reviewed TV dramas and some sci-fi.  Meanwhile, our bottom rated shows include goofball comedies.\n",
    "\n",
    "*Note, because of random initialization in the weights, results on subsequent runs may differ slightly.*\n",
    "\n",
    "Let's confirm that we no longer have almost perfect correlation in recommendations with user #7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pb = create_payload(7, nb_customer, nb_products, product_index)\n",
    "response = fm_predictor.predict(x_pb)\n",
    "predictions_user7 = [round(r['score'], 2) for r in json.loads(response)['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions_df['prediction'], np.array(predictions_user7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "So far, we have developed a deep learning model to predict customer ratings but the model could be improved further by various techniques. In this section, let's see if tuning the hyper-parameters of Factorization Machine is going to make the model any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/train/'.format(bucket)\n",
    "s3_train_path = 's3://{}/prepare/train/train.protobuf'.format(bucket)\n",
    "s3_val_path = 's3://{}/prepare/validate/validate.protobuf'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create the estimator with Factorization Machines container similar to how we defined in training the model. Also, set the initial hyper-parameters that we know worked before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_estimator = sagemaker.estimator.Estimator(container,\n",
    "                                   role, \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c5.4xlarge',\n",
    "                                   output_path=output_location,\n",
    "                                   sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_estimator.set_hyperparameters(\n",
    "    feature_dim=feature_dim,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=200,\n",
    "    num_factors=512,\n",
    "    bias_lr=0.02,\n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find best hyperparameters with Sagemaker's Automatic Model Tuning. Following hyperparameters will be tuned\n",
    "    - ***factors_lr:*** The learning rate for factorization terms.\n",
    "    - ***factors_init_sigma:*** The standard deviation for initialization of factorization terms. Takes effect if factors_init_method is set to normal.\n",
    "    \n",
    "\n",
    "- Define the hyperparameter tuning ranges to be searched and set the objective metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges=  {\n",
    "    \"factors_lr\": ContinuousParameter(0.0001, 0.2),\n",
    "    \"factors_init_sigma\": ContinuousParameter(0.0001, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have our ranges defined we want to define our success metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"test:rmse\"\n",
    "objective_type = \"Minimize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start hyperparameter tuning job with the ranges defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_tuner = HyperparameterTuner(\n",
    "    estimator=fm_estimator,\n",
    "    objective_metric_name=objective_metric_name, \n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    objective_type=objective_type,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_prefix = time.strftime(\"%Y%m%d-%H%M%S\", time.gmtime())\n",
    "fm_tuner_job_name = 'hpo-fm-' + timestamp_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_tuner.fit({'train': s3_train_path, 'test': s3_val_path}, job_name=fm_tuner_job_name, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Track hyperparameter tuning job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check current status of hyperparameter tuning job\n",
    "tuning_job_result = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=fm_tuner_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "    \n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze Hyper-Parameter Tuning Job Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug-in the training job name and metrics to be captured\n",
    "fm_tuner_analytics = HyperparameterTuningJobAnalytics(hyperparameter_tuning_job_name=fm_tuner_job_name)\n",
    "df_fm_tuner_metrics = fm_tuner_analytics.dataframe()\n",
    "df_fm_tuner_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze using seaborn\n",
    "plt = df_fm_tuner_metrics.plot(kind='line', figsize=(12,5), x='TrainingStartTime', \n",
    "                             y='FinalObjectiveValue', \n",
    "                             style='b.', legend=False)\n",
    "plt.set_ylabel(objective_metric_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Factorization Machine Model after Hyper-Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fm_tuner_job_name: \" + fm_tuner_job_name)\n",
    "fm_tuner = HyperparameterTuner.attach(fm_tuner_job_name)\n",
    "\n",
    "fm_tuner_analytics = HyperparameterTuningJobAnalytics(hyperparameter_tuning_job_name=fm_tuner_job_name)\n",
    "df_fm_tuner_metrics = fm_tuner_analytics.dataframe()\n",
    "\n",
    "fm_best_model_name = fm_tuner.best_training_job()\n",
    "print(\"fm_best_model_name: \" + fm_best_model_name)\n",
    "\n",
    "fm_model_info = smclient.describe_training_job(TrainingJobName=fm_best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fm_tuner_metrics[df_fm_tuner_metrics['TrainingJobName']==fm_best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's evaluate the results with the best training job from hyper-parameter tuning job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deploy the endpoint using hyper-parameter tuning job and test the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = sagemaker.estimator.Estimator.attach(fm_best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-run the cells in Batch Inference and Evaluation section to evaluate the performance of the model with tuned hyper-parameters. \n",
    "\n",
    "Assuming batch inference is carried out, let's calculate predictions for our test dataset and see if we do better than the training job with default hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', np.mean((test_Y - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer ratings.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (video genres, historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_contains = ['-fm-', 'factorization-machines-']\n",
    "for name in endpoint_name_contains:\n",
    "    endpoints = smclient.list_endpoints(NameContains=name, StatusEquals='InService')\n",
    "    endpoint_names = [r['EndpointName'] for r in endpoints['Endpoints']]\n",
    "    for endpoint_name in endpoint_names:\n",
    "        print(\"Deleting endpoint: \" + endpoint_name)\n",
    "        smclient.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
